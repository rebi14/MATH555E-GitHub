{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #1\n",
    "\n",
    "There are four text files in the data folder\n",
    "\n",
    "* Atatürk's \"Nutuk\" in Turkish\n",
    "* Dicken's novel \"Great Expectations\" in English\n",
    "* Flauberts' novel \"Madam Bovary\" in French\n",
    "* A text file `unknown.txt` in an unknown language\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Calculate how many times each character (letter) appear in each text.\n",
    "* Calculate the character distributions, i.e. using the character counts, calculate the probability of each character appearing in the text.\n",
    "* Find the set of characters common to all three texts.\n",
    "* Using the common set and the KL-divergence, show that each language have different character distributions.\n",
    "* Determine the language of the text file `unknown.txt` KL-divergence measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many times each character (letter) appear in each text.\n",
    "from collections import Counter\n",
    "def CalculateCharacter(txtname):\n",
    "    total_count = 0\n",
    "    file = open(txtname, \"r\", errors ='ignore') # open file\n",
    "    charcount = {} #dictionary to hold char counts\n",
    "    validchars = \"abcçdefgğhıijklmnoöpqrsştuüvyz\" # only these counted\n",
    " \n",
    "    print(\": Letter : Frequency :\")\n",
    " \n",
    "    for i in range(97,123): # lowercase range\n",
    "        c = (chr(i)) # the chars a-z\n",
    "        charcount[c] = 0 # initialize count\n",
    " \n",
    "    for line in file:\n",
    "        words = line.split(\" \") # line into words\n",
    "        for word in words:  # words into chars\n",
    "            chars = list(word) #convert word into a char list\n",
    "            for c in chars:  # process chars\n",
    "                if c.isalpha():  # only alpha allowd\n",
    "                    if c.isupper():\n",
    "                        c = c.lower()  # if char is upper convert to lower\n",
    "                    if c in validchars: # if in valid char set\n",
    "                        charcount[c] += 1 # increment count\n",
    "                    \n",
    "        \n",
    "    \n",
    "    print(charcount) # print list                   \n",
    "    file.close() # close file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Letter : Frequency :\n",
      "{'a': 119069, 'b': 29401, 'c': 10939, 'd': 45880, 'e': 113769, 'f': 7498, 'g': 14561, 'h': 12848, 'i': 94615, 'j': 163, 'k': 48302, 'l': 75077, 'm': 70970, 'n': 78316, 'o': 20304, 'p': 7380, 'q': 1, 'r': 74210, 's': 30823, 't': 42763, 'u': 39595, 'v': 15632, 'w': 0, 'x': 0, 'y': 29511, 'z': 15602}\n"
     ]
    }
   ],
   "source": [
    "CalculateCharacter(\"ataturk_nutuk.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Letter : Frequency :\n",
      "{'a': 64021, 'b': 12451, 'c': 17625, 'd': 37182, 'e': 93720, 'f': 16130, 'g': 16871, 'h': 48738, 'i': 55801, 'j': 1759, 'k': 7680, 'l': 28564, 'm': 22906, 'n': 53781, 'o': 61277, 'p': 13417, 'q': 710, 'r': 42122, 's': 45936, 't': 70355, 'u': 22257, 'v': 6896, 'w': 0, 'x': 0, 'y': 16410, 'z': 166}\n"
     ]
    }
   ],
   "source": [
    "CalculateCharacter(\"dickens_great_expectations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Letter : Frequency :\n",
      "{'a': 46611, 'b': 6245, 'c': 16344, 'd': 20056, 'e': 79764, 'f': 5880, 'g': 5190, 'h': 6177, 'i': 40356, 'j': 2052, 'k': 154, 'l': 35195, 'm': 15786, 'n': 36296, 'o': 29092, 'p': 14651, 'q': 5709, 'r': 35857, 's': 44074, 't': 40678, 'u': 33588, 'v': 8791, 'w': 0, 'x': 0, 'y': 2029, 'z': 678}\n"
     ]
    }
   ],
   "source": [
    "CalculateCharacter(\"flaubert_madame_bovary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9e in position 7249: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-cea23c4dc749>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ataturk_nutuk.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;31m# For this to work, make sure the words don't end with any punctuation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;31m# If any words end with a punctuation, take a look into re.sub()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1254.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9e in position 7249: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "#Calculate the character distributions, i.e. using the character counts, calculate the probability of each character appearing in the text.\n",
    "new_dict = {}\n",
    "\n",
    "# This is to open the file to get the count of all words:\n",
    "filename = 'ataturk_nutuk.txt'    \n",
    "with open(filename, \"r\") as fp:\n",
    "        for line in fp:\n",
    "            # For this to work, make sure the words don't end with any punctuation.\n",
    "            # If any words end with a punctuation, take a look into re.sub()\n",
    "            words = line.split()\n",
    "    \n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "    \n",
    "                if word not in new_dict:\n",
    "                  new_dict[word] = 1\n",
    "                else:\n",
    "                  new_dict[word] += 1\n",
    "    \n",
    "    # This is to calculate the count of all words:\n",
    "total_words = sum(new_dict.values())\n",
    "\n",
    "    # This is used to get the probability of each word.\n",
    "output_file = 'a.txt'\n",
    "with open(output_file, \"w\") as fs:\n",
    "        # The dictionary is set as: {dictionary_name[key] : value}\n",
    "        for key, value in sorted(new_dict.items()):\n",
    "            probability = value / total_words\n",
    "            fs.write(key + \": \" + str(probability) + \"\\n\")   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "\n",
    "For this question consider the [Car Evaluation Data Set](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) from UCI. Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data) to the dataset.\n",
    "\n",
    "Make [contingency tables](https://en.wikipedia.org/wiki/Contingency_table#:~:text=In%20statistics%2C%20a%20contingency%20table,%2C%20engineering%2C%20and%20scientific%20research.) of the columns (using [`crosstab`](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html) function from [pandas](https://pandas.pydata.org)) and figure out which pairs of columns are dependent and independent. Explain your result using statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #3\n",
    "\n",
    "For this question, use [Default of Credit Card Clients Data Set]() from UCI. Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls) to the dataset.\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Inspect the dataset.\n",
    "* Would it be appropriate to form a linear regression model to predict the `default payment next month` variable? Explain.\n",
    "* Form a [contingency table](https://en.wikipedia.org/wiki/Contingency_table#:~:text=In%20statistics%2C%20a%20contingency%20table,%2C%20engineering%2C%20and%20scientific%20research.) of the columns `SEX` vs `default payment next month` and `EDUCATION` vs `default payment next month`.\n",
    "* Are there statistically verifiable relationships between credit card defaults, the gender of and the education level the borrower? Which is stronger? Quantify your analysis using [Chi Square Test](https://en.wikipedia.org/wiki/Chi-squared_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #4\n",
    "\n",
    "For this question, use the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris) from UCI.  Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data) to the dataset.\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Form a [K-NN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) model for this dataset.\n",
    "* Test your model on random samples of your data and calculate its accuracy.\n",
    "* Repeat your calculation 100 times and give an interval of accuracy values leaving the best 2.5% and worst 2.5% accuracy values.\n",
    "* Is there a better way of doing this without repeating the calculation 100 times? Explain.\n",
    "* Find the best parameter $k$ for your dataset for the K-NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #5\n",
    "\n",
    "For this question, we are going to use [Concrete Slump Test Dataset](https://archive.ics.uci.edu/ml/datasets/Concrete+Slump+Test) from UCI. Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data) to the dataset.\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Form three separate linear regression model for the following dependent variables:\n",
    "\n",
    "  - SLUMP (cm)\n",
    "  - FLOW (cm)\n",
    "  - 28-day Compressive Strength (Mpa)\n",
    "  \n",
    "* Compare how well these models fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
