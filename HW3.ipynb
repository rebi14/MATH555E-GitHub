{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #1\n",
    "\n",
    "For this question you will use [Olivetti Face Dataset](https://scikit-learn.org/0.19/datasets/olivetti_faces.html).\n",
    "\n",
    "### Part 1\n",
    "\n",
    "1. Split your dataset as train and test subset. But make sure that each test set contains exactly one random image from each distinct individual. This means, you will have to write your own train_test_split function for this dataset.\n",
    "\n",
    "2. Construct an SVM model on your train set, and test its accuracy on your test set. For this part, the images viewed as integer vectors of length 4096 are independent variables while the id number of the person that picture belongs to is the dependent variable. In other words, you are trying to construct an SVM model that recognizes individuals based on their pictures.\n",
    "\n",
    "3. Repeat Step 2 ten times.\n",
    "\n",
    "4. Calculate the mean accuracy and get 95% confidence interval using the t-test.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Do the same things you did in Part 1 but with a multinomial regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#olivetti face dataset içinde 40 farklı insana ait her kişinin 10'ar tane yüz resmi bulunmaktadır.\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import numpy as np\n",
    "data=np.load(\"olivetti_faces.npy\")\n",
    "target=np.load(\"olivetti_faces_target.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.30991736, 0.3677686 , 0.41735536, ..., 0.37190083,\n",
       "         0.3305785 , 0.30578512],\n",
       "        [0.3429752 , 0.40495867, 0.43801653, ..., 0.37190083,\n",
       "         0.338843  , 0.3140496 ],\n",
       "        [0.3429752 , 0.41735536, 0.45041323, ..., 0.38016528,\n",
       "         0.338843  , 0.29752067],\n",
       "        ...,\n",
       "        [0.21487603, 0.20661157, 0.2231405 , ..., 0.15289256,\n",
       "         0.16528925, 0.17355372],\n",
       "        [0.20247933, 0.2107438 , 0.2107438 , ..., 0.14876033,\n",
       "         0.16115703, 0.16528925],\n",
       "        [0.20247933, 0.20661157, 0.20247933, ..., 0.15289256,\n",
       "         0.16115703, 0.1570248 ]],\n",
       "\n",
       "       [[0.45454547, 0.47107437, 0.5123967 , ..., 0.19008264,\n",
       "         0.18595041, 0.18595041],\n",
       "        [0.446281  , 0.48347107, 0.5206612 , ..., 0.21487603,\n",
       "         0.2107438 , 0.2107438 ],\n",
       "        [0.49586776, 0.5165289 , 0.53305787, ..., 0.20247933,\n",
       "         0.20661157, 0.20661157],\n",
       "        ...,\n",
       "        [0.77272725, 0.78099173, 0.7933884 , ..., 0.1446281 ,\n",
       "         0.1446281 , 0.1446281 ],\n",
       "        [0.77272725, 0.7768595 , 0.7892562 , ..., 0.13636364,\n",
       "         0.13636364, 0.13636364],\n",
       "        [0.7644628 , 0.7892562 , 0.78099173, ..., 0.15289256,\n",
       "         0.15289256, 0.15289256]],\n",
       "\n",
       "       [[0.3181818 , 0.40082645, 0.49173555, ..., 0.40082645,\n",
       "         0.3553719 , 0.30991736],\n",
       "        [0.30991736, 0.3966942 , 0.47933885, ..., 0.40495867,\n",
       "         0.37603307, 0.30165288],\n",
       "        [0.26859504, 0.34710744, 0.45454547, ..., 0.3966942 ,\n",
       "         0.37190083, 0.30991736],\n",
       "        ...,\n",
       "        [0.1322314 , 0.09917355, 0.08264463, ..., 0.13636364,\n",
       "         0.14876033, 0.15289256],\n",
       "        [0.11570248, 0.09504132, 0.0785124 , ..., 0.1446281 ,\n",
       "         0.1446281 , 0.1570248 ],\n",
       "        [0.11157025, 0.09090909, 0.0785124 , ..., 0.14049587,\n",
       "         0.14876033, 0.15289256]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.5       , 0.53305787, 0.607438  , ..., 0.28512397,\n",
       "         0.23966943, 0.21487603],\n",
       "        [0.49173555, 0.5413223 , 0.60330576, ..., 0.29752067,\n",
       "         0.20247933, 0.20661157],\n",
       "        [0.46694216, 0.55785125, 0.6198347 , ..., 0.29752067,\n",
       "         0.17768595, 0.18595041],\n",
       "        ...,\n",
       "        [0.03305785, 0.46280992, 0.5289256 , ..., 0.17355372,\n",
       "         0.17355372, 0.1694215 ],\n",
       "        [0.1570248 , 0.5247934 , 0.53305787, ..., 0.16528925,\n",
       "         0.1570248 , 0.18595041],\n",
       "        [0.45454547, 0.5206612 , 0.53305787, ..., 0.17768595,\n",
       "         0.14876033, 0.19008264]],\n",
       "\n",
       "       [[0.21487603, 0.21900827, 0.21900827, ..., 0.71487606,\n",
       "         0.71487606, 0.6942149 ],\n",
       "        [0.20247933, 0.20661157, 0.20661157, ..., 0.7107438 ,\n",
       "         0.7066116 , 0.6942149 ],\n",
       "        [0.2107438 , 0.20661157, 0.20661157, ..., 0.6859504 ,\n",
       "         0.69008267, 0.6942149 ],\n",
       "        ...,\n",
       "        [0.2644628 , 0.25619835, 0.2603306 , ..., 0.5413223 ,\n",
       "         0.57438016, 0.59090906],\n",
       "        [0.26859504, 0.2644628 , 0.26859504, ..., 0.56198347,\n",
       "         0.58264464, 0.59504133],\n",
       "        [0.27272728, 0.26859504, 0.27272728, ..., 0.57438016,\n",
       "         0.59090906, 0.60330576]],\n",
       "\n",
       "       [[0.5165289 , 0.46280992, 0.28099173, ..., 0.5785124 ,\n",
       "         0.5413223 , 0.60330576],\n",
       "        [0.5165289 , 0.45041323, 0.29338843, ..., 0.58264464,\n",
       "         0.553719  , 0.5785124 ],\n",
       "        [0.5165289 , 0.44214877, 0.29338843, ..., 0.59917355,\n",
       "         0.5785124 , 0.54545456],\n",
       "        ...,\n",
       "        [0.39256197, 0.41322315, 0.38842976, ..., 0.33471075,\n",
       "         0.37190083, 0.3966942 ],\n",
       "        [0.39256197, 0.38429752, 0.40495867, ..., 0.3305785 ,\n",
       "         0.35950413, 0.37603307],\n",
       "        [0.3677686 , 0.40495867, 0.3966942 , ..., 0.35950413,\n",
       "         0.3553719 , 0.38429752]]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
       "       13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "       28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30,\n",
       "       30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32,\n",
       "       32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "       34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35,\n",
       "       35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39,\n",
       "       39, 39, 39, 39, 39, 39, 39, 39, 39])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400 images in the dataset\n",
      "There are 40 unique targets in the dataset\n",
      "Size of each image is 64x64\n",
      "Pixel values were scaled to [0,1] interval. e.g:[0.30991736 0.3677686  0.41735536 0.44214877]\n",
      "unique target number: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} images in the dataset\".format(len(data)))\n",
    "print(\"There are {} unique targets in the dataset\".format(len(np.unique(target))))\n",
    "print(\"Size of each image is {}x{}\".format(data.shape[1],data.shape[2]))\n",
    "print(\"Pixel values were scaled to [0,1] interval. e.g:{}\".format(data[0][0,:4]))\n",
    "print(\"unique target number:\",np.unique(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (400, 4096)\n"
     ]
    }
   ],
   "source": [
    "#We reshape images for machine learnig  model\n",
    "X=data.reshape((data.shape[0],data.shape[1]*data.shape[2]))\n",
    "print(\"X shape:\",X.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (280, 4096)\n",
      "y_train shape:(280,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, target, test_size=0.3, stratify=target, random_state=0)\n",
    "print(\"X_train shape:\",X_train.shape)\n",
    "print(\"y_train shape:{}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of SVM model is {} : 92.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "classifier_svm = svm.SVC(kernel='linear')\n",
    "classifier_svm.fit(X_train , y_train )\n",
    "print('The accuracy of SVM model is {} : ' + str(round(classifier_svm.score(X_test, y_test),2)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "model_log = logReg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h_train = logReg.predict(X_train)\n",
    "y_h_test = logReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         3\n",
      "           1       1.00      0.33      0.50         3\n",
      "           2       0.75      1.00      0.86         3\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      0.67      0.80         3\n",
      "           5       0.43      1.00      0.60         3\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       1.00      1.00      1.00         3\n",
      "           9       1.00      0.67      0.80         3\n",
      "          10       1.00      0.67      0.80         3\n",
      "          11       1.00      0.67      0.80         3\n",
      "          12       1.00      0.67      0.80         3\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       0.60      1.00      0.75         3\n",
      "          15       0.75      1.00      0.86         3\n",
      "          16       1.00      1.00      1.00         3\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00         3\n",
      "          19       1.00      1.00      1.00         3\n",
      "          20       1.00      0.67      0.80         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       0.75      1.00      0.86         3\n",
      "          23       0.75      1.00      0.86         3\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       1.00      0.33      0.50         3\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         3\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       0.75      1.00      0.86         3\n",
      "          31       0.00      0.00      0.00         3\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       0.43      1.00      0.60         3\n",
      "          34       0.75      1.00      0.86         3\n",
      "          35       1.00      1.00      1.00         3\n",
      "          36       1.00      1.00      1.00         3\n",
      "          37       0.67      0.67      0.67         3\n",
      "          38       1.00      1.00      1.00         3\n",
      "          39       0.67      0.67      0.67         3\n",
      "\n",
      "    accuracy                           0.85       120\n",
      "   macro avg       0.88      0.85      0.84       120\n",
      "weighted avg       0.88      0.85      0.84       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabia.ozer\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_h_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is :85.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = logReg.score(X_test, y_test)\n",
    "print('The accuracy of Logistic Regression model is :' + str(result*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of MultinomialNB model is :82.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB().fit(X_train,y_train)\n",
    "result = model.score(X_test, y_test)\n",
    "print('The accuracy of MultinomialNB model is :' + str(result*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "\n",
    "For this question you will use [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Convert the dataset into numerical data using [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) from SciKitLearn's `sklearn.feature_extraction.text` module. Make sure that you also record whether a given movie review is positive or negative or neutral. Calling on `CountVectorizer` on individual entries is not going to be enough. You will have to do some preliminary work. Read the documentation carefully.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Using the numerical data you constructed in Part 1, construct an LDA model to see if data projects into a 2D space with clear separation. Analyze your result.\n",
    "\n",
    "\n",
    "### Part 3\n",
    "\n",
    "Using the numerical data you constructed in Part 1, \n",
    "\n",
    "1. Split the data as train and test using SciKitLearn's `train_test_split` function.\n",
    "2. Form a multiclass SVM model on the train set and test its accuracy.\n",
    "3. Repeat a small number of times and get mean accuracy and its error band.\n",
    "\n",
    "### Part 4\n",
    "\n",
    "Repeat Part 2 using multinomial regression models instead of SVM.\n",
    "\n",
    "### Part 5\n",
    "\n",
    "Using the numerical data you constructed in Part 1, \n",
    "\n",
    "1. Construct an PCA model and look at the eigenvalues from largest to smallest. \n",
    "2. How many dimensions needed to capture 90% of the variation of the data? (Read the documentation of [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) form SciKitLearn)\n",
    "3. Transform your data using the result you obtained in Step 2.\n",
    "4. Construct an SVM model on the new dataset you constructed and cross-validate it.\n",
    "5. Compare your result with the result you obtained in Part 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "print(dataset.shape)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    25000\n",
       "positive    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = dataset['review'].iloc[1]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets arrange data\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(review, 'html.parser')\n",
    "review = soup.get_text()\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'wonderful',\n",
       " 'little',\n",
       " 'production',\n",
       " 'the',\n",
       " 'filming',\n",
       " 'technique',\n",
       " 'is',\n",
       " 'very',\n",
       " 'unassuming',\n",
       " 'very',\n",
       " 'old',\n",
       " 'time',\n",
       " 'bbc',\n",
       " 'fashion',\n",
       " 'and',\n",
       " 'gives',\n",
       " 'a',\n",
       " 'comforting',\n",
       " 'and',\n",
       " 'sometimes',\n",
       " 'discomforting',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'realism',\n",
       " 'to',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'piece',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'are',\n",
       " 'extremely',\n",
       " 'well',\n",
       " 'chosen',\n",
       " 'michael',\n",
       " 'sheen',\n",
       " 'not',\n",
       " 'only',\n",
       " 'has',\n",
       " 'got',\n",
       " 'all',\n",
       " 'the',\n",
       " 'polari',\n",
       " 'but',\n",
       " 'he',\n",
       " 'has',\n",
       " 'all',\n",
       " 'the',\n",
       " 'voices',\n",
       " 'down',\n",
       " 'pat',\n",
       " 'too',\n",
       " 'you',\n",
       " 'can',\n",
       " 'truly',\n",
       " 'see',\n",
       " 'the',\n",
       " 'seamless',\n",
       " 'editing',\n",
       " 'guided',\n",
       " 'by',\n",
       " 'the',\n",
       " 'references',\n",
       " 'to',\n",
       " 'williams',\n",
       " 'diary',\n",
       " 'entries',\n",
       " 'not',\n",
       " 'only',\n",
       " 'is',\n",
       " 'it',\n",
       " 'well',\n",
       " 'worth',\n",
       " 'the',\n",
       " 'watching',\n",
       " 'but',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'terrificly',\n",
       " 'written',\n",
       " 'and',\n",
       " 'performed',\n",
       " 'piece',\n",
       " 'a',\n",
       " 'masterful',\n",
       " 'production',\n",
       " 'about',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'great',\n",
       " 'master',\n",
       " 's',\n",
       " 'of',\n",
       " 'comedy',\n",
       " 'and',\n",
       " 'his',\n",
       " 'life',\n",
       " 'the',\n",
       " 'realism',\n",
       " 'really',\n",
       " 'comes',\n",
       " 'home',\n",
       " 'with',\n",
       " 'the',\n",
       " 'little',\n",
       " 'things',\n",
       " 'the',\n",
       " 'fantasy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guard',\n",
       " 'which',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'use',\n",
       " 'the',\n",
       " 'traditional',\n",
       " 'dream',\n",
       " 'techniques',\n",
       " 'remains',\n",
       " 'solid',\n",
       " 'then',\n",
       " 'disappears',\n",
       " 'it',\n",
       " 'plays',\n",
       " 'on',\n",
       " 'our',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'our',\n",
       " 'senses',\n",
       " 'particularly',\n",
       " 'with',\n",
       " 'the',\n",
       " 'scenes',\n",
       " 'concerning',\n",
       " 'orton',\n",
       " 'and',\n",
       " 'halliwell',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sets',\n",
       " 'particularly',\n",
       " 'of',\n",
       " 'their',\n",
       " 'flat',\n",
       " 'with',\n",
       " 'halliwell',\n",
       " 's',\n",
       " 'murals',\n",
       " 'decorating',\n",
       " 'every',\n",
       " 'surface',\n",
       " 'are',\n",
       " 'terribly',\n",
       " 'well',\n",
       " 'done']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "review = re.sub('\\[[^]]*/]',' ',review)\n",
    "review = re.sub('[^a-zA-Z]',' ',review)\n",
    "review =  review.lower()\n",
    "review = review.split()\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wonderful',\n",
       " 'little',\n",
       " 'production',\n",
       " 'filming',\n",
       " 'technique',\n",
       " 'unassuming',\n",
       " 'old',\n",
       " 'time',\n",
       " 'bbc',\n",
       " 'fashion',\n",
       " 'gives',\n",
       " 'comforting',\n",
       " 'sometimes',\n",
       " 'discomforting',\n",
       " 'sense',\n",
       " 'realism',\n",
       " 'entire',\n",
       " 'piece',\n",
       " 'actors',\n",
       " 'extremely',\n",
       " 'well',\n",
       " 'chosen',\n",
       " 'michael',\n",
       " 'sheen',\n",
       " 'got',\n",
       " 'polari',\n",
       " 'voices',\n",
       " 'pat',\n",
       " 'truly',\n",
       " 'see',\n",
       " 'seamless',\n",
       " 'editing',\n",
       " 'guided',\n",
       " 'references',\n",
       " 'williams',\n",
       " 'diary',\n",
       " 'entries',\n",
       " 'well',\n",
       " 'worth',\n",
       " 'watching',\n",
       " 'terrificly',\n",
       " 'written',\n",
       " 'performed',\n",
       " 'piece',\n",
       " 'masterful',\n",
       " 'production',\n",
       " 'one',\n",
       " 'great',\n",
       " 'master',\n",
       " 'comedy',\n",
       " 'life',\n",
       " 'realism',\n",
       " 'really',\n",
       " 'comes',\n",
       " 'home',\n",
       " 'little',\n",
       " 'things',\n",
       " 'fantasy',\n",
       " 'guard',\n",
       " 'rather',\n",
       " 'use',\n",
       " 'traditional',\n",
       " 'dream',\n",
       " 'techniques',\n",
       " 'remains',\n",
       " 'solid',\n",
       " 'disappears',\n",
       " 'plays',\n",
       " 'knowledge',\n",
       " 'senses',\n",
       " 'particularly',\n",
       " 'scenes',\n",
       " 'concerning',\n",
       " 'orton',\n",
       " 'halliwell',\n",
       " 'sets',\n",
       " 'particularly',\n",
       " 'flat',\n",
       " 'halliwell',\n",
       " 'murals',\n",
       " 'decorating',\n",
       " 'every',\n",
       " 'surface',\n",
       " 'terribly',\n",
       " 'well',\n",
       " 'done']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wonderful little production filming technique unassuming old time bbc fashion give comforting sometimes discomforting sense realism entire piece actor extremely well chosen michael sheen got polari voice pat truly see seamless editing guided reference williams diary entry well worth watching terrificly written performed piece masterful production one great master comedy life realism really come home little thing fantasy guard rather use traditional dream technique remains solid disappears play knowledge sens particularly scene concerning orton halliwell set particularly flat halliwell mural decorating every surface terribly well done']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "review_stemmer = [stemmer.stem(word) for word in review]\n",
    "review_stemmer = ' '.join(review_stemmer)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "review_lemmatize = [lemmatizer.lemmatize(word) for word in review]\n",
    "review_lemmatize = ' '.join(review_lemmatize)\n",
    "review_lemmatize\n",
    "corpus = []\n",
    "corpus.append(review_lemmatize)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1,\n",
       "        1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09712859, 0.09712859, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.19425717, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.19425717, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.09712859, 0.09712859, 0.19425717,\n",
       "        0.09712859, 0.09712859, 0.19425717, 0.09712859, 0.09712859,\n",
       "        0.19425717, 0.09712859, 0.19425717, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.19425717, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.09712859, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859, 0.29138576, 0.09712859, 0.09712859,\n",
       "        0.09712859, 0.09712859]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf  =  TfidfVectorizer()\n",
    "\n",
    "review_tfidf = tfidf.fit_transform(corpus).toarray()\n",
    "review_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['review'], dataset['sentiment'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace({'positive':1, 'negative':0})\n",
    "y_test = y_test.replace({'positive':1, 'negative':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-183d76f1e210>:8: FutureWarning: Possible nested set at position 2\n",
      "  review = re.sub('/[[^]]*/]',' ',review)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "corpus_train = []\n",
    "corpus_test = []\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    soup = BeautifulSoup(x_train.iloc[i],'html.parser')\n",
    "    review = soup.get_text()\n",
    "    review = re.sub('/[[^]]*/]',' ',review)\n",
    "    review = re.sub('[^a-zA-Z]',' ',review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    lm  = WordNetLemmatizer()\n",
    "    review = [lm.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus_train.append(review)\n",
    "\n",
    "\n",
    "for j in range(x_test.shape[0]):\n",
    "    soup = BeautifulSoup(x_test.iloc[j],'html.parser')\n",
    "    review = soup.get_text()\n",
    "    review = re.sub('/[[^]]*/]',' ',review)\n",
    "    review = re.sub('[^a-zA-Z]',' ',review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [lm.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus_test.append(review)\n",
    "    \n",
    "    \n",
    "tfidf = TfidfVectorizer(ngram_range = (1, 3))\n",
    "\n",
    "tfidf_train = tfidf.fit_transform(corpus_train)\n",
    "tfidf_test = tfidf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svc = LinearSVC(C=0.5, random_state=42)\n",
    "linear_svc.fit(tfidf_train,y_train)\n",
    "\n",
    "predict = linear_svc.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print('Classification Report: \\n', classification_report(y_test, predict, target_names = ['Negative', 'Positive']))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_test, predict))\n",
    "print('Accuracy score: \\n', accuracy_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data=pd.read_csv('IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text normalization\n",
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing html strips and noise text\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized train and test reviews\n",
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_train_reviews[0]\n",
    "norm_test_reviews=imdb_data.review[40000:]\n",
    "norm_test_reviews[45005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test_reviews)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)\n",
    "\n",
    "#Spliting the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]\n",
    "print(train_sentiments)\n",
    "print(test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
    "print(lr_bow)\n",
    "#Fitting the model for tfidf features\n",
    "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "##Predicting the model for tfidf features\n",
    "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
    "print(\"lr_bow_score :\",lr_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score :\",lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
